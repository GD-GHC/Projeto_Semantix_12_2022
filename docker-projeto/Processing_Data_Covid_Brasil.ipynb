{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    Observação:<br>\n",
    "    As indicações de comandos a serem executados no terminal consideram que o diretório corrente é o que contém os recursos do cluster Docker, incluindo o arquivo docker-compose.yml.\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    Comandos de Terminal:<br>\n",
    "    &emsp; Reiniciar/Remover recursos relacionados ao cluster Docker:\n",
    "</h2>\n",
    "<font size=3>\n",
    "    <ol>\n",
    "        <li>docker image ls</li><br>\n",
    "        <li>docker compose down</li>\n",
    "        <li>docker volume prune</li>\n",
    "        <li>docker system prune --all</li>\n",
    "        <li>docker image ls</li><br>\n",
    "        <li>sudo sysctl -w vm.max_map_count=65536</li>\n",
    "    </ol>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    Comandos de Terminal:<br>\n",
    "    &emsp; Reiniciar/Criar e listar recursos do cluster Docker para atividades do projeto:<br>\n",
    "    &emsp; Após a execução o resultado na tela deve ser equivalente ao conteúdo da tabela abaixo:\n",
    "</h2>\n",
    "\n",
    "<font size=3>\n",
    "    <ol>\n",
    "        <li>docker compose up -d</li>\n",
    "        <li>docker compose ps</li>\n",
    "    </ol>\n",
    "</font>\n",
    "\n",
    "<br>\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th>NAME</th>\n",
    "        <th>COMMAND</th>\n",
    "        <th>SERVICE</th>\n",
    "        <th>STATUS</th>\n",
    "        <th>PORTS</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>datanode</th>\n",
    "        <th>\"/entrypoint.sh /run…\"</th>\n",
    "        <th>datanode</th>\n",
    "        <th>running (healthy)</th>\n",
    "        <th>0.0.0.0:50075->50075/tcp, :::50075->50075/tcp</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>docker-projeto-elasticsearch-1</th>\n",
    "        <th>\"/tini -- /usr/local…\"</th>\n",
    "        <th>elasticsearch</th>\n",
    "        <th>running</th>\n",
    "        <th>0.0.0.0:9200->9200/tcp, :::9200->9200/tcp, 9300/tcp</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>docker-projeto-kibana-1</th>\n",
    "        <th>\"/usr/local/bin/dumb…\"</th>\n",
    "        <th>kibana</th>\n",
    "        <th>running</th>\n",
    "        <th>0.0.0.0:5601->5601/tcp, :::5601->5601/tcp</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>docker-projeto-logstash-1</th>\n",
    "        <th>\"/usr/local/bin/dock…\"</th>\n",
    "        <th>logstash</th>\n",
    "        <th>running</th>\n",
    "        <th>0.0.0.0:5044->5044/tcp, :::5044->5044/tcp, 0.0.0.0:9600->9600/tcp, :::9600->9600/tcp</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>hive-metastore-postgresql</th>\n",
    "        <th>\"/docker-entrypoint.…\"</th>\n",
    "        <th>hive-metastore-postgresql</th>\n",
    "        <th>running</th>\n",
    "        <th>5432/tcp</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>hive-server</th>\n",
    "        <th>\"entrypoint.sh /bin/…\"</th>\n",
    "        <th>hive-server</th>\n",
    "        <th>running</th>\n",
    "        <th>0.0.0.0:10000->10000/tcp, :::10000->10000/tcp, 10002/tcp</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>hive_metastore</th>\n",
    "        <th>\"entrypoint.sh /opt/…\"</th>\n",
    "        <th>hive-metastore</th>\n",
    "        <th>running</th>\n",
    "        <th>10000/tcp, 0.0.0.0:9083->9083/tcp, :::9083->9083/tcp, 10002/tcp</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>jupyter-spark</th>\n",
    "        <th>\"/opt/docker/bin/ent…\"</th>\n",
    "        <th>jupyter-spark</th>\n",
    "        <th>running</th>\n",
    "        <th>0.0.0.0:4040-4043->4040-4043/tcp, :::4040-4043->4040-4043/tcp, 0.0.0.0:8889->8889/tcp, :::8889->8889/tcp, 8899/tcp</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>kafka</th>\n",
    "        <th>\"start-kafka.sh\"</th>\n",
    "        <th>kafka</th>\n",
    "        <th>running</th>\n",
    "        <th>0.0.0.0:9092->9092/tcp, :::9092->9092/tcp</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>kafkamanager</th>\n",
    "        <th>\"kafka-manager-2.0.0…\"</th>\n",
    "        <th>kamanager</th>\n",
    "        <th>running</th>\n",
    "        <th>0.0.0.0:9000->9000/tcp, :::9000->9000/tcp</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>namenode</th>\n",
    "        <th>\"/entrypoint.sh /run…\"</th>\n",
    "        <th>namenode</th>\n",
    "        <th>running (healthy)</th>\n",
    "        <th>0.0.0.0:50071->50070/tcp, :::50071->50070/tcp</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <th>zookeeper</th>\n",
    "        <th>\"/bin/sh -c '/usr/sb…\"</th>\n",
    "        <th>zookeeper</th>\n",
    "        <th>running</th>\n",
    "        <th>22/tcp, 2888/tcp, 3888/tcp, 0.0.0.0:2181->2181/tcp, :::2181->2181/tcp</th>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    Comandos de Terminal:<br>\n",
    "    &emsp; Correção para problema do nó elasticsearch do cluster cair frequentemente.<br>\n",
    "    &emsp; Após a execução o resultado na tela deve ser equivalente ao conteúdo da tabela acima.\n",
    "</h2>\n",
    "<font size=3>\n",
    "    <ol>\n",
    "        <li>docker compose down</li><br>\n",
    "        <li>sudo sysctl -w vm.max_map_count=262144</li><br>\n",
    "        <li>docker compose up -d</li>\n",
    "        <li>docker compose ps</li>\n",
    "    </ol>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    Endereços para abrir no navegador web para verificar funcionamento do Jupyter-Notebook e Kibana (Elasticsearch), respectivamente:\n",
    "</h2>\n",
    "<font size=3>\n",
    "    <ol>\n",
    "        <li>localhost:8889</li>\n",
    "        <li>localhost:5601</li><br>\n",
    "        <li>Fazer upload do arquivo $<$Diretório-recursos-cluster$>$/Processing_Data_Covid_Brasil.ipynb no Jupyter-Notebook.</li>\n",
    "    </ol>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    Comandos de Terminal:<br>\n",
    "    &emsp; Copiar e verificar arquivo .jar para o nó do Spark para salvar arquivos parquet:\n",
    "</h2>\n",
    "<font size=3>\n",
    "    <ol>\n",
    "        <li>docker cp parquet-hadoop-bundle-1.6.0.jar jupyter-spark:/opt/spark/jars</li>\n",
    "        <li>docker exec -it jupyter-spark bash</li>\n",
    "        <li>ls opt/spark/jars/</li>\n",
    "        <li>exit</li>\n",
    "        <li>docker compose stop</li>\n",
    "        <li>docker compose start</li>\n",
    "        <li>docker compose ps</li>\n",
    "    </ol>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    Comandos de Terminal:<br>\n",
    "    &emsp; Copiar dados para processamento para diretório acessível pelo cluster Docker:\n",
    "</h2>\n",
    "<font size=3>\n",
    "    <ol>\n",
    "        <li>sudo cp -r data_covid_brasil_11_2022/ input/</li><br>\n",
    "        <li>docker exec -it namenode bash</li>\n",
    "        <li>ls input/</li>\n",
    "        <li>ls input/data_covid_brasil_11_2022/</li>\n",
    "        <li>exit</li>\n",
    "    </ol>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    Sequência de comandos para execução no Jupyter-Notebook:<br>\n",
    "    &emsp; Preparar estrutura de diretório para atividades do projeto:\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    1 - Remover diretório raiz de usuário, caso exista no cluster:\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/student': No such file or directory\n",
      "Found 1 items\n",
      "drwxr-xr-x   - root supergroup          0 2022-12-05 19:38 /user/hive\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/student\n",
    "!hdfs dfs -ls /user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    2 - Consultar tabelas de Banco de Dados Hive possivelmente existentes:\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\n",
      "drwxrwxr-x   - root supergroup          0 2022-12-05 19:39 /tmp\n",
      "drwxr-xr-x   - root supergroup          0 2022-12-05 19:38 /user\n",
      "Found 1 items\n",
      "drwxr-xr-x   - root supergroup          0 2022-12-05 19:38 /user/hive\n",
      "Found 1 items\n",
      "drwxrwxr-x   - root supergroup          0 2022-12-05 19:38 /user/hive/warehouse\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /\n",
    "!hdfs dfs -ls /user\n",
    "!hdfs dfs -ls /user/hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "!hdfs dfs -ls /user/hive/warehouse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    3 - Criar estrutura de dados necessária para o projeto:\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-12-05 19:38 /user/hive\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-12-05 19:49 /user/student\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mkdir /user/student\n",
    "!hdfs dfs -ls /user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 items\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-12-05 19:49 /user/student/original_data\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mkdir /user/student/original_data\n",
    "!hdfs dfs -ls /user/student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-12-05 19:49 /user/student/original_data\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-12-05 19:50 /user/student/prepared_data\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mkdir /user/student/prepared_data\n",
    "!hdfs dfs -ls /user/student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 items\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-12-05 19:49 /user/student/original_data\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-12-05 19:50 /user/student/prepared_data\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-12-05 19:50 /user/student/result_data\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mkdir /user/student/result_data\n",
    "!hdfs dfs -ls /user/student"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-12-05 19:50 /user/student/hive_database\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-12-05 19:49 /user/student/original_data\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-12-05 19:50 /user/student/prepared_data\r\n",
      "drwxr-xr-x   - root supergroup          0 2022-12-05 19:50 /user/student/result_data\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mkdir /user/student/hive_database\n",
    "!hdfs dfs -ls /user/student"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    Comandos de Terminal:<br>\n",
    "    &emsp; Exercício 1 Nível Básico:<br>\n",
    "    &emsp; Copiar dados para processamento para o HDFS e verificar a cópia:\n",
    "</h2>\n",
    "<font size=3>\n",
    "    <ol>\n",
    "        <li>docker exec -it namenode bash</li>\n",
    "        <li>hdfs dfs -put /input/data_covid_brasil_11_2022/\\*.csv /user/student/original_data</li>\n",
    "        <li>hdfs dfs -ls /user/student/original_data</li>\n",
    "        <li>exit</li>\n",
    "    </ol>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    Comandos de Terminal:<br>\n",
    "    &emsp; Criar Banco de Dados Hive para inclusão das tabelas necessárias:\n",
    "</h2>\n",
    "<font size=3>\n",
    "    <ol>\n",
    "        <li>docker exec -it hive-server bash</li>\n",
    "        <li>hive</li>\n",
    "        <li>show databases;</li>\n",
    "        <li>create database my_database location \"/user/student/hive_database/\";</li>\n",
    "        <li>show databases;</li>\n",
    "        <li>desc database my_database;</li>\n",
    "        <li>use my_database;</li>\n",
    "        <li>show tables;</li>\n",
    "        <li>create table test(id int);</li>\n",
    "        <li>show tables;</li>\n",
    "        <li>drop table test;</li>\n",
    "        <li>show tables;</li>\n",
    "        <li>exit;</li>\n",
    "        <li>exit</li>\n",
    "    </ol>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    Sequência de comandos para execução no Jupyter-Notebook:<br>\n",
    "    &emsp; Carregar elementos necessários.<br>\n",
    "    &emsp; Ler e apresentar informações sobre os dados para processamento:\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    1 - Importar os módulos necessários para o projeto:\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as psf\n",
    "import pyspark.sql.types as pst\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    2 - Apresentar os dados da sessão Spark e do contexto Spark:\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jupyter-spark:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f8320676fd0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://jupyter-spark:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    3 - Listar os Bancos de Dados Hive, ajustar o Banco de Dados corrente e apresentar as suas tabelas:\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Database(name='default', description='Default Hive database', locationUri='hdfs://namenode:8020/user/hive/warehouse'),\n",
       " Database(name='my_database', description='', locationUri='hdfs://namenode:8020/user/student/hive_database')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listDatabases()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'my_database'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.setCurrentDatabase('my_database')\n",
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    4 - Preparar e apresentra o schema para os dados a serem lidos:\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(regiao,StringType,true),StructField(estado,StringType,true),StructField(municipio,StringType,true),StructField(coduf,IntegerType,true),StructField(codmun,IntegerType,true),StructField(codRegiaoSaude,IntegerType,true),StructField(nomeRegiaoSaude,StringType,true),StructField(data,StringType,true),StructField(semanaEpi,IntegerType,true),StructField(populacaoTCU2019,IntegerType,true),StructField(casosAcumulado,IntegerType,true),StructField(casosNovos,IntegerType,true),StructField(obitosAcumulado,IntegerType,true),StructField(obitosNovos,IntegerType,true),StructField(Recuperadosnovos,IntegerType,true),StructField(emAcompanhamentoNovos,IntegerType,true),StructField(interior/metropolitana,IntegerType,true)))\n"
     ]
    }
   ],
   "source": [
    "data_schema = [\n",
    "    pst.StructField('regiao', pst.StringType()),\n",
    "    pst.StructField('estado', pst.StringType()),\n",
    "    pst.StructField('municipio', pst.StringType()),\n",
    "    pst.StructField('coduf', pst.IntegerType()),\n",
    "    pst.StructField('codmun', pst.IntegerType()),\n",
    "    pst.StructField('codRegiaoSaude', pst.IntegerType()),\n",
    "    pst.StructField('nomeRegiaoSaude', pst.StringType()),\n",
    "    pst.StructField('data', pst.StringType()),\n",
    "    pst.StructField('semanaEpi', pst.IntegerType()),\n",
    "    pst.StructField('populacaoTCU2019', pst.IntegerType()),\n",
    "    pst.StructField('casosAcumulado', pst.IntegerType()),\n",
    "    pst.StructField('casosNovos', pst.IntegerType()),\n",
    "    pst.StructField('obitosAcumulado', pst.IntegerType()),\n",
    "    pst.StructField('obitosNovos', pst.IntegerType()),\n",
    "    pst.StructField('Recuperadosnovos', pst.IntegerType()),\n",
    "    pst.StructField('emAcompanhamentoNovos', pst.IntegerType()),\n",
    "    pst.StructField('interior/metropolitana', pst.IntegerType()),\n",
    "]\n",
    "data_schema = pst.StructType(fields=data_schema)\n",
    "\n",
    "print(data_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    5 - Ler os dados referente ao ano de infomação definido e apresentar informações da estrutura:\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "year_analysis = 2022\n",
    "data_path = 'hdfs:///user/student/original_data/HIST_PAINEL_COVIDBR_' + str(year_analysis) + '_Parte*.csv'\n",
    "\n",
    "data_table = spark.read.csv(path=data_path, header=True, sep=';', schema=data_schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of Read Data:\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "Schema of Read Data:\n",
      "root\n",
      " |-- regiao: string (nullable = true)\n",
      " |-- estado: string (nullable = true)\n",
      " |-- municipio: string (nullable = true)\n",
      " |-- coduf: integer (nullable = true)\n",
      " |-- codmun: integer (nullable = true)\n",
      " |-- codRegiaoSaude: integer (nullable = true)\n",
      " |-- nomeRegiaoSaude: string (nullable = true)\n",
      " |-- data: string (nullable = true)\n",
      " |-- semanaEpi: integer (nullable = true)\n",
      " |-- populacaoTCU2019: integer (nullable = true)\n",
      " |-- casosAcumulado: integer (nullable = true)\n",
      " |-- casosNovos: integer (nullable = true)\n",
      " |-- obitosAcumulado: integer (nullable = true)\n",
      " |-- obitosNovos: integer (nullable = true)\n",
      " |-- Recuperadosnovos: integer (nullable = true)\n",
      " |-- emAcompanhamentoNovos: integer (nullable = true)\n",
      " |-- interior/metropolitana: integer (nullable = true)\n",
      "\n",
      "None\n",
      "Columns of Read Data:\n",
      "['regiao', 'estado', 'municipio', 'coduf', 'codmun', 'codRegiaoSaude', 'nomeRegiaoSaude', 'data', 'semanaEpi', 'populacaoTCU2019', 'casosAcumulado', 'casosNovos', 'obitosAcumulado', 'obitosNovos', 'Recuperadosnovos', 'emAcompanhamentoNovos', 'interior/metropolitana']\n",
      "Total of Samples in Read Data:\n",
      "1871127\n",
      "First Sample of Read Data:\n",
      "Row(regiao='Brasil', estado=None, municipio=None, coduf=76, codmun=None, codRegiaoSaude=None, nomeRegiaoSaude=None, data='2022-01-01', semanaEpi=52, populacaoTCU2019=210147125, casosAcumulado=22291507, casosNovos=3986, obitosAcumulado=619105, obitosNovos=49, Recuperadosnovos=21581668, emAcompanhamentoNovos=90734, interior/metropolitana=None)\n"
     ]
    }
   ],
   "source": [
    "print('Type of Read Data:')\n",
    "print(type(data_table))\n",
    "\n",
    "print('Schema of Read Data:')\n",
    "print(data_table.printSchema())\n",
    "\n",
    "print('Columns of Read Data:')\n",
    "print(data_table.columns)\n",
    "\n",
    "print('Total of Samples in Read Data:')\n",
    "print(data_table.count())\n",
    "\n",
    "print('First Sample of Read Data:')\n",
    "print(data_table.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    6 - Alterar nome de coluna dos dados por conta de caracter especial \"/\" na estrutura original:\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(regiao='Brasil', estado=None, municipio=None, coduf=76, codmun=None, codRegiaoSaude=None, nomeRegiaoSaude=None, data='2022-01-01', semanaEpi=52, populacaoTCU2019=210147125, casosAcumulado=22291507, casosNovos=3986, obitosAcumulado=619105, obitosNovos=49, Recuperadosnovos=21581668, emAcompanhamentoNovos=90734, interiorMetropolitana=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_table = data_table.withColumn(colName='interiorMetropolitana', col=psf.col('interior/metropolitana'))\n",
    "data_table = data_table.drop(psf.col('interior/metropolitana'))\n",
    "\n",
    "data_table.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    7 - Alterar formato dos dados de coluna referente a data:\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(data='2022-01-01')\n",
      "Row(data='01/01/2022')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Row(regiao='Brasil', estado=None, municipio=None, coduf=76, codmun=None, codRegiaoSaude=None, nomeRegiaoSaude=None, data='01/01/2022', semanaEpi=52, populacaoTCU2019=210147125, casosAcumulado=22291507, casosNovos=3986, obitosAcumulado=619105, obitosNovos=49, Recuperadosnovos=21581668, emAcompanhamentoNovos=90734, interiorMetropolitana=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data_table.select('data').first())\n",
    "\n",
    "data_table = data_table.withColumn(colName='new_data', col=psf.unix_timestamp(timestamp='data', format='yyyy-MM-dd'))\n",
    "data_table = data_table.withColumn(colName='new_data', col=psf.from_unixtime(timestamp='new_data', format='dd/MM/yyyy'))\n",
    "data_table = data_table.withColumn(colName='data', col=psf.col('new_data'))\n",
    "data_table = data_table.drop(psf.col('new_data'))\n",
    "\n",
    "print(data_table.select('data').first())\n",
    "\n",
    "data_table.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    8 - Exercício 2 Nível Básico:<br>\n",
    "    &emsp; Salvar dados como tabela Hive particionada por municípios:\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table.write.saveAsTable(name='exercicio_2_basico', partitionBy='municipio', format='csv', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    9 - Listar tabelas no Banco de Dados Hive após salvar dados:\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='exercicio_2_basico', database='my_database', description=None, tableType='MANAGED', isTemporary=False)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    Comandos de Terminal:<br>\n",
    "    &emsp; Consultar no HDFS os dados salvos em tabela do Banco de Dados Hive: \n",
    "</h2>\n",
    "<font size=3>\n",
    "    <ol>\n",
    "        <li>docker exec -it namenode bash</li>\n",
    "        <li>hdfs dfs -ls /user/student/hive_database</li>\n",
    "        <li>hdfs dfs -ls /user/student/hive_database/exercicio_2_basico</li>\n",
    "        <li>hdfs dfs -ls /user/student/hive_database/exercicio_2_basico/municipio=S?o\\ Paulo</li>\n",
    "        <li>exit</li>\n",
    "    </ol>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    Sequência de comandos para execução no Jupyter-Notebook:<br>\n",
    "    &emsp; Identificar e tratar os valores nulos entre os dados:\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    1 - Identificar a apresentar as colunas que contém valores nulos:\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total of Null Values by Column:\n",
      "1) regiao: 0\n",
      "2) estado: 333\n",
      "3) municipio: 16317\n",
      "4) coduf: 0\n",
      "5) codmun: 9324\n",
      "6) codRegiaoSaude: 16317\n",
      "7) nomeRegiaoSaude: 16317\n",
      "8) data: 0\n",
      "9) semanaEpi: 0\n",
      "10) populacaoTCU2019: 6993\n",
      "11) casosAcumulado: 1\n",
      "12) casosNovos: 0\n",
      "13) obitosAcumulado: 0\n",
      "14) obitosNovos: 0\n",
      "15) Recuperadosnovos: 1870795\n",
      "16) emAcompanhamentoNovos: 1870795\n",
      "17) interiorMetropolitana: 16317\n",
      "\n",
      "\n",
      "Columns With Null Values:\n",
      "['estado', 'municipio', 'codmun', 'codRegiaoSaude', 'nomeRegiaoSaude', 'populacaoTCU2019', 'casosAcumulado', 'Recuperadosnovos', 'emAcompanhamentoNovos', 'interiorMetropolitana']\n",
      "Total of Columns With Null Values:\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(\"Total of Null Values by Column:\")\n",
    "\n",
    "columns_null_values = []\n",
    "for i, c in enumerate(data_table.columns):\n",
    "    value = data_table.filter(condition=data_table[c].isNull()).count()\n",
    "    if value > 0:\n",
    "        columns_null_values.append(c)\n",
    "    print(str(i + 1) + ') ' + c + ': ' + str(value))\n",
    "\n",
    "print('\\n')\n",
    "print('Columns With Null Values:')\n",
    "print(columns_null_values)\n",
    "print('Total of Columns With Null Values:')\n",
    "print(len(columns_null_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    2 - Identificar e apresentar os tipo de dados de todas as colunas do conjunto de dados:\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Type of Columns:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'regiao': 'string',\n",
       " 'estado': 'string',\n",
       " 'municipio': 'string',\n",
       " 'coduf': 'int',\n",
       " 'codmun': 'int',\n",
       " 'codRegiaoSaude': 'int',\n",
       " 'nomeRegiaoSaude': 'string',\n",
       " 'data': 'string',\n",
       " 'semanaEpi': 'int',\n",
       " 'populacaoTCU2019': 'int',\n",
       " 'casosAcumulado': 'int',\n",
       " 'casosNovos': 'int',\n",
       " 'obitosAcumulado': 'int',\n",
       " 'obitosNovos': 'int',\n",
       " 'Recuperadosnovos': 'int',\n",
       " 'emAcompanhamentoNovos': 'int',\n",
       " 'interiorMetropolitana': 'int'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns_type = {}\n",
    "for c in data_table.dtypes:\n",
    "    columns_type[c[0]] = c[1]\n",
    "\n",
    "print('Data Type of Columns:')\n",
    "columns_type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    3 - Identificar/Separar e apresentar as colunas com valores nulos de acordo com o tipo:\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "String Columns With Null Values:\n",
      "['estado', 'municipio', 'nomeRegiaoSaude']\n",
      "Integer Columns With Null Values:\n",
      "['codmun', 'codRegiaoSaude', 'populacaoTCU2019', 'casosAcumulado', 'Recuperadosnovos', 'emAcompanhamentoNovos', 'interiorMetropolitana']\n"
     ]
    }
   ],
   "source": [
    "string_columns_null = []\n",
    "int_columns_null = []\n",
    "for c in columns_null_values:\n",
    "    if columns_type[c] == 'string':\n",
    "        string_columns_null.append(c)\n",
    "    elif columns_type[c] == 'int':\n",
    "        int_columns_null.append(c)\n",
    "\n",
    "print('String Columns With Null Values:')\n",
    "print(string_columns_null)\n",
    "print('Integer Columns With Null Values:')\n",
    "print(int_columns_null)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    4 - Apresentar o total de valores distintos e nulos em cada coluna que contém valores nulos:\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) estado:\n",
      "\t Total of Values: 28\n",
      "\t Not Null Values: 27\n",
      "\t Null Values....: 1\n",
      "2) municipio:\n",
      "\t Total of Values: 5298\n",
      "\t Not Null Values: 5297\n",
      "\t Null Values....: 1\n",
      "3) codmun:\n",
      "\t Total of Values: 5592\n",
      "\t Not Null Values: 5591\n",
      "\t Null Values....: 1\n",
      "4) codRegiaoSaude:\n",
      "\t Total of Values: 451\n",
      "\t Not Null Values: 450\n",
      "\t Null Values....: 1\n",
      "5) nomeRegiaoSaude:\n",
      "\t Total of Values: 441\n",
      "\t Not Null Values: 440\n",
      "\t Null Values....: 1\n",
      "6) populacaoTCU2019:\n",
      "\t Total of Values: 5105\n",
      "\t Not Null Values: 5104\n",
      "\t Null Values....: 1\n",
      "7) casosAcumulado:\n",
      "\t Total of Values: 68086\n",
      "\t Not Null Values: 68085\n",
      "\t Null Values....: 1\n",
      "8) Recuperadosnovos:\n",
      "\t Total of Values: 333\n",
      "\t Not Null Values: 332\n",
      "\t Null Values....: 1\n",
      "9) emAcompanhamentoNovos:\n",
      "\t Total of Values: 333\n",
      "\t Not Null Values: 332\n",
      "\t Null Values....: 1\n",
      "10) interiorMetropolitana:\n",
      "\t Total of Values: 3\n",
      "\t Not Null Values: 2\n",
      "\t Null Values....: 1\n"
     ]
    }
   ],
   "source": [
    "for i, c in enumerate(columns_null_values):\n",
    "    temp_values = data_table.select(c).distinct().collect()\n",
    "    values = []\n",
    "    for temp_value in temp_values:\n",
    "        value = temp_value.asDict()[c]\n",
    "        values.append(value)\n",
    "    if None in values:\n",
    "        text = str(i + 1) + ') ' + c + ':\\n'\n",
    "        text += '\\t Total of Values: ' + str(len(values)) + '\\n'\n",
    "        total_null = len(np.where(np.array(values).astype(str) == 'None')[0])\n",
    "        text += '\\t Not Null Values: ' + str(len(values) - total_null) + '\\n'\n",
    "        text += '\\t Null Values....: ' + str(total_null)\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    5 - Identificar e apresentar se colunas com valores nulos possuem valores pretendidos para substituição:\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NullValue not in estado\n",
      "NullValue not in municipio\n",
      "-1 not in codmun\n",
      "-1 not in codRegiaoSaude\n",
      "NullValue not in nomeRegiaoSaude\n",
      "-1 not in populacaoTCU2019\n",
      "-1 not in casosAcumulado\n",
      "-1 not in Recuperadosnovos\n",
      "-1 not in emAcompanhamentoNovos\n",
      "-1 not in interiorMetropolitana\n"
     ]
    }
   ],
   "source": [
    "for c in columns_null_values:\n",
    "    temp_values = data_table.select(c).distinct().collect()\n",
    "    values = []\n",
    "    for temp_value in temp_values:\n",
    "        value = temp_value.asDict()[c]\n",
    "        values.append(value)\n",
    "    if None in values:\n",
    "        values.remove(None)\n",
    "    if columns_type[c] == 'string':\n",
    "        if not ('NullValue' in values):\n",
    "            print(\"NullValue not in \" + c)\n",
    "    elif columns_type[c] == 'int':\n",
    "        if not ((-1) in values):\n",
    "            print(\"-1 not in \" + c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    6 - Substituir valores nulos de acordo com o tipo de dado das colunas e apresentar dado resultante:\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(regiao='Brasil', estado='NullValue', municipio='NullValue', coduf=76, codmun=-1, codRegiaoSaude=-1, nomeRegiaoSaude='NullValue', data='01/01/2022', semanaEpi=52, populacaoTCU2019=210147125, casosAcumulado=22291507, casosNovos=3986, obitosAcumulado=619105, obitosNovos=49, Recuperadosnovos=21581668, emAcompanhamentoNovos=90734, interiorMetropolitana=-1)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_table = data_table.na.fill(subset=string_columns_null, value='NullValue')\n",
    "data_table = data_table.na.fill(subset=int_columns_null, value=-1)\n",
    "\n",
    "data_table.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    7 - Salvar os dados após ajuste de colunas e tratamento dos valores nulos:\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_table.write.csv(path='hdfs:///user/student/prepared_data/data_changed_columns_without_null_values', header=True, sep=';', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    Comandos de Terminal:<br>\n",
    "    &emsp; Consultar dados preparados salvos no HDFS:\n",
    "</h2>\n",
    "<font size=3>\n",
    "    <ol>\n",
    "        <li>docker exec -it namenode bash</li>\n",
    "        <li>hdfs dfs -ls /user/student/prepared_data</li>\n",
    "        <li>hdfs dfs -ls /user/student/prepared_data/data_changed_columns_without_null_values</li>\n",
    "        <li>exit</li>\n",
    "    </ol>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    Sequência de comandos para execução no Jupyter-Notebook:<br>\n",
    "    &emsp; Ler os dados preparados salvos no HDFS para processamento:\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    1 - Preparar e apresentar o schema dos dados a serem lidos:\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(regiao,StringType,true),StructField(estado,StringType,true),StructField(municipio,StringType,true),StructField(coduf,IntegerType,true),StructField(codmun,IntegerType,true),StructField(codRegiaoSaude,IntegerType,true),StructField(nomeRegiaoSaude,StringType,true),StructField(data,StringType,true),StructField(semanaEpi,IntegerType,true),StructField(populacaoTCU2019,IntegerType,true),StructField(casosAcumulado,IntegerType,true),StructField(casosNovos,IntegerType,true),StructField(obitosAcumulado,IntegerType,true),StructField(obitosNovos,IntegerType,true),StructField(Recuperadosnovos,IntegerType,true),StructField(emAcompanhamentoNovos,IntegerType,true),StructField(interiorMetropolitana,IntegerType,true)))\n"
     ]
    }
   ],
   "source": [
    "data_schema = [\n",
    "    pst.StructField('regiao', pst.StringType()),\n",
    "    pst.StructField('estado', pst.StringType()),\n",
    "    pst.StructField('municipio', pst.StringType()),\n",
    "    pst.StructField('coduf', pst.IntegerType()),\n",
    "    pst.StructField('codmun', pst.IntegerType()),\n",
    "    pst.StructField('codRegiaoSaude', pst.IntegerType()),\n",
    "    pst.StructField('nomeRegiaoSaude', pst.StringType()),\n",
    "    pst.StructField('data', pst.StringType()),\n",
    "    pst.StructField('semanaEpi', pst.IntegerType()),\n",
    "    pst.StructField('populacaoTCU2019', pst.IntegerType()),\n",
    "    pst.StructField('casosAcumulado', pst.IntegerType()),\n",
    "    pst.StructField('casosNovos', pst.IntegerType()),\n",
    "    pst.StructField('obitosAcumulado', pst.IntegerType()),\n",
    "    pst.StructField('obitosNovos', pst.IntegerType()),\n",
    "    pst.StructField('Recuperadosnovos', pst.IntegerType()),\n",
    "    pst.StructField('emAcompanhamentoNovos', pst.IntegerType()),\n",
    "    pst.StructField('interiorMetropolitana', pst.IntegerType()),\n",
    "]\n",
    "data_schema = pst.StructType(fields=data_schema)\n",
    "\n",
    "print(data_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    2 - Ler e os dados para processamento e apresentar informações da estrutura:\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(regiao='Brasil', estado='NullValue', municipio='NullValue', coduf=76, codmun=-1, codRegiaoSaude=-1, nomeRegiaoSaude='NullValue', data='01/01/2022', semanaEpi=52, populacaoTCU2019=210147125, casosAcumulado=22291507, casosNovos=3986, obitosAcumulado=619105, obitosNovos=49, Recuperadosnovos=21581668, emAcompanhamentoNovos=90734, interiorMetropolitana=-1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path = 'hdfs:///user/student/prepared_data/data_changed_columns_without_null_values/'\n",
    "\n",
    "data_table = spark.read.csv(path=data_path, header=True, sep=';', schema=data_schema)\n",
    "\n",
    "data_table.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type of Saved Prepapred Data:\n",
      "<class 'pyspark.sql.dataframe.DataFrame'>\n",
      "Schema of Saved Prepapred Data:\n",
      "root\n",
      " |-- regiao: string (nullable = true)\n",
      " |-- estado: string (nullable = true)\n",
      " |-- municipio: string (nullable = true)\n",
      " |-- coduf: integer (nullable = true)\n",
      " |-- codmun: integer (nullable = true)\n",
      " |-- codRegiaoSaude: integer (nullable = true)\n",
      " |-- nomeRegiaoSaude: string (nullable = true)\n",
      " |-- data: string (nullable = true)\n",
      " |-- semanaEpi: integer (nullable = true)\n",
      " |-- populacaoTCU2019: integer (nullable = true)\n",
      " |-- casosAcumulado: integer (nullable = true)\n",
      " |-- casosNovos: integer (nullable = true)\n",
      " |-- obitosAcumulado: integer (nullable = true)\n",
      " |-- obitosNovos: integer (nullable = true)\n",
      " |-- Recuperadosnovos: integer (nullable = true)\n",
      " |-- emAcompanhamentoNovos: integer (nullable = true)\n",
      " |-- interiorMetropolitana: integer (nullable = true)\n",
      "\n",
      "None\n",
      "Columns of Saved Prepapred Data:\n",
      "['regiao', 'estado', 'municipio', 'coduf', 'codmun', 'codRegiaoSaude', 'nomeRegiaoSaude', 'data', 'semanaEpi', 'populacaoTCU2019', 'casosAcumulado', 'casosNovos', 'obitosAcumulado', 'obitosNovos', 'Recuperadosnovos', 'emAcompanhamentoNovos', 'interiorMetropolitana']\n",
      "Total of Samples in Saved Prepapred Data:\n",
      "1871127\n",
      "First Sample of Saved Prepapred Data:\n",
      "Row(regiao='Brasil', estado='NullValue', municipio='NullValue', coduf=76, codmun=-1, codRegiaoSaude=-1, nomeRegiaoSaude='NullValue', data='01/01/2022', semanaEpi=52, populacaoTCU2019=210147125, casosAcumulado=22291507, casosNovos=3986, obitosAcumulado=619105, obitosNovos=49, Recuperadosnovos=21581668, emAcompanhamentoNovos=90734, interiorMetropolitana=-1)\n"
     ]
    }
   ],
   "source": [
    "print('Type of Saved Prepapred Data:')\n",
    "print(type(data_table))\n",
    "\n",
    "print('Schema of Saved Prepapred Data:')\n",
    "print(data_table.printSchema())\n",
    "\n",
    "print('Columns of Saved Prepapred Data:')\n",
    "print(data_table.columns)\n",
    "\n",
    "print('Total of Samples in Saved Prepapred Data:')\n",
    "print(data_table.count())\n",
    "\n",
    "print('First Sample of Saved Prepapred Data:')\n",
    "print(data_table.first())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    Sequência de comandos para execução no Jupyter-Notebook:<br>\n",
    "    &emsp; Parte Exercício 3 e Exercício 4 Nível Básico:\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    1 - Ler os dados para visualização 1 do Exercício 3 Nível Básico, aplicando filtros necessários:\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Recuperadosnovos=21581668, emAcompanhamentoNovos=90734)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_data = data_table.where(condition=(psf.col('regiao') == 'Brasil') & (psf.col('Recuperadosnovos') > -1) & (psf.col('emAcompanhamentoNovos') > -1))\\\n",
    "                        .select(psf.col('Recuperadosnovos'), psf.col('emAcompanhamentoNovos'))\n",
    "\n",
    "temp_data.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    2 - Preparar e apresentar schema dos dados da visualização 1:\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(Recuperadosnovos,IntegerType,true),StructField(emAcompanhamentoNovos,IntegerType,true)))\n"
     ]
    }
   ],
   "source": [
    "temp_data_schema = []\n",
    "for c in temp_data.columns:\n",
    "    temp_data_schema.append(pst.StructField(name=c, dataType=pst.IntegerType()))\n",
    "temp_data_schema = pst.StructType(fields=temp_data_schema)\n",
    "\n",
    "print(temp_data_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    3 - Criar um novo DataFrame com os dados para a visualização:<br>\n",
    "    &emsp; Parte Exercício 3 Nível Básico (Visualização 1):\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(Recuperadosnovos=34235867, emAcompanhamentoNovos=302067)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_data = [temp_data.collect()[-1]]\n",
    "temp_data = spark.createDataFrame(data=temp_data, schema=temp_data_schema)\n",
    "\n",
    "temp_data.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    4 - Salvar os dados da Visualização 1 como tabela em Banco de Dados Hive:<br>\n",
    "    &emsp; Exercício 4 Nível Básico:\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data.write.saveAsTable(name='exercicio_3_basico_visualizacao_1', format='parquet', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    5 - Consultar a tabela em Banco de Dados Hive após salvar os dados:\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='exercicio_2_basico', database='my_database', description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='exercicio_3_basico_visualizacao_1', database='my_database', description=None, tableType='MANAGED', isTemporary=False)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(Recuperadosnovos=34235867, emAcompanhamentoNovos=302067)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('SELECT * FROM exercicio_3_basico_visualizacao_1').collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    Sequência de comandos para execução no Jupyter-Notebook:<br>\n",
    "    &emsp; Parte Exercício 3 e Exercício 5 Nível Básico:\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    1 - Ler os dados para a visualização 2 do Exercício 3 Nível Básico aplicando o filtro necessário:\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(casosAcumulado=22291507, casosNovos=3986, populacaoTCU2019=210147125)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_data = data_table.where(condition=(psf.col('regiao') == 'Brasil') & (psf.col('casosAcumulado') > -1) & (psf.col('casosNovos') > -1) & (psf.col('populacaoTCU2019') > -1))\\\n",
    "                        .select(psf.col('casosAcumulado'), psf.col('casosNovos'), psf.col('populacaoTCU2019'))\n",
    "\n",
    "temp_data.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    2 - Incluir nova coluna entre os dados de acordo com a visualização 2 e alterar o seu tipo de dado:\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(casosAcumulado=22291507, casosNovos=3986, populacaoTCU2019=210147125, incidencia=10607.572265625)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_data = temp_data.withColumn(colName='incidencia', col=(psf.col('casosAcumulado').cast('float')/psf.col('populacaoTCU2019').cast('float'))*100000.0)\n",
    "temp_data = temp_data.withColumn(colName='incidencia', col=psf.col('incidencia').cast('float'))\n",
    "\n",
    "temp_data.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    3 - Preparar e apresentar o schema dos dados da visualização 2:\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(casosAcumulado,IntegerType,true),StructField(casosNovos,IntegerType,true),StructField(populacaoTCU2019,IntegerType,true),StructField(incidencia,FloatType,true)))\n"
     ]
    }
   ],
   "source": [
    "temp_data_schema = []\n",
    "for c in temp_data.columns[0:3]:\n",
    "    temp_data_schema.append(pst.StructField(name=c, dataType=pst.IntegerType()))\n",
    "temp_data_schema.append(pst.StructField(name='incidencia', dataType=pst.FloatType()))\n",
    "temp_data_schema = pst.StructType(fields=temp_data_schema)\n",
    "\n",
    "print(temp_data_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    4 - Criar um novo DataFrame com os dados para a visualização:<br>\n",
    "    &emsp; Parte Exercício 3 Nível Básico (Visualização 2):\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(casosAcumulado=35227599, casosNovos=39013, incidencia=16763.3046875)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_data = [temp_data.collect()[-1]]\n",
    "temp_data = spark.createDataFrame(data=temp_data, schema=temp_data_schema)\n",
    "\n",
    "temp_data = temp_data.select(psf.col('casosAcumulado'), psf.col('casosNovos'), psf.col('incidencia'))\n",
    "\n",
    "temp_data.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    5 - Salvar os dados como arquivo parquet e compressão snappy:<br>\n",
    "    &emsp; Exercício 5 Nível Básico:\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data.write.parquet(path='hdfs:///user/student/result_data/exercicio_3_basico_visualizacao_2', compression='snappy', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    6 - Ler e apresentar os dados após salvar:\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(casosAcumulado=35227599, casosNovos=39013, incidencia=16763.3046875)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_data = spark.read.parquet('hdfs:///user/student/result_data/exercicio_3_basico_visualizacao_2')\n",
    "\n",
    "temp_data.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    Comandos de Terminal:<br>\n",
    "    &emsp; Consultar os dados salvos no HDFS:\n",
    "</h2>\n",
    "<font size=3>\n",
    "    <ol>\n",
    "        <li>docker exec -it namenode bash</li>\n",
    "        <li>hdfs dfs -ls /user/student/result_data</li>\n",
    "        <li>hdfs dfs -ls /user/student/result_data/exercicio_3_basico_visualizacao_2</li>\n",
    "        <li>exit</li>\n",
    "    </ol>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    Sequência de comandos para execução no Jupyter-Notebook:<br>\n",
    "    &emsp; Parte Exercício 3 e Exercício 6 Nivel Básico:\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    1 - Ler os dados para a visualização 3 do Exercício 3 Nível Básico aplicando o filtro necessário: \n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(casosAcumulado=22291507, obitosAcumulado=619105, obitosNovos=49, populacaoTCU2019=210147125)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_data = data_table.where(condition=(psf.col('regiao') == 'Brasil') & (psf.col('casosAcumulado') > -1) & (psf.col('obitosAcumulado') > -1) & (psf.col('obitosNovos') > -1) & (psf.col('populacaoTCU2019') > -1))\\\n",
    "                        .select(psf.col('casosAcumulado'), psf.col('obitosAcumulado'), psf.col('obitosNovos'), psf.col('populacaoTCU2019'))\n",
    "\n",
    "temp_data.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    2 - Incluir novas colunas entre os dados de acordo com a visualização 3:\n",
    "</3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(casosAcumulado=22291507, obitosAcumulado=619105, obitosNovos=49, populacaoTCU2019=210147125, letalidade=2.777313232421875, mortalidade=294.6054992675781)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_data = temp_data.withColumn(colName='letalidade', col=(psf.col('obitosAcumulado').cast('float')/psf.col('casosAcumulado').cast('float'))*100.0)\n",
    "temp_data = temp_data.withColumn(colName='letalidade', col=psf.col('letalidade').cast('float'))\n",
    "\n",
    "temp_data = temp_data.withColumn(colName='mortalidade', col=(psf.col('obitosAcumulado').cast('float')/psf.col('populacaoTCU2019').cast('float'))*100000.0)\n",
    "temp_data = temp_data.withColumn(colName='mortalidade', col=psf.col('mortalidade').cast('float'))\n",
    "\n",
    "temp_data.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    3 - Preparar e apresentar o schema dos dados da visualização 3: \n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(casosAcumulado,IntegerType,true),StructField(obitosAcumulado,IntegerType,true),StructField(obitosNovos,IntegerType,true),StructField(populacaoTCU2019,IntegerType,true),StructField(letalidade,FloatType,true),StructField(mortalidade,FloatType,true)))\n"
     ]
    }
   ],
   "source": [
    "temp_data_schema = []\n",
    "for c in temp_data.columns[0:4]:\n",
    "    temp_data_schema.append(pst.StructField(name=c, dataType=pst.IntegerType()))\n",
    "temp_data_schema.append(pst.StructField(name='letalidade', dataType=pst.FloatType()))\n",
    "temp_data_schema.append(pst.StructField(name='mortalidade', dataType=pst.FloatType()))\n",
    "temp_data_schema = pst.StructType(fields=temp_data_schema)\n",
    "\n",
    "print(temp_data_schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    4 - Criar um novo DataFrame com os dados para a visualização:<br>\n",
    "    &emsp; Parte Exercício 3 Nível Básico (Visualização 3):\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(obitosAcumulado=689665, obitosNovos=129, letalidade='1.96', mortalidade='328.18')"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_data = [temp_data.collect()[-1]]\n",
    "temp_data = spark.createDataFrame(data=temp_data, schema=temp_data_schema)\n",
    "\n",
    "temp_data = temp_data.select(psf.col('obitosAcumulado'), psf.col('obitosNovos'), psf.format_number(col=psf.col('letalidade'), d=2).alias('letalidade'), psf.format_number(col=psf.col('mortalidade'), d=2).alias('mortalidade'))\n",
    "\n",
    "temp_data.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    5 - Incluir coluna value para enviar dados para tópico Kafka:\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(obitosAcumulado=689665, obitosNovos=129, letalidade='1.96', mortalidade='328.18', value='689665 - 129 - 1.96 - 328.18')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_data = temp_data.withColumn(colName='value', col=psf.concat(psf.col('obitosAcumulado'), psf.lit(' - '), psf.col('obitosNovos'), psf.lit(' - '), psf.col('letalidade'), psf.lit(' - '), psf.col('mortalidade')))\n",
    "\n",
    "temp_data.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    Comandos de Terminal:<br>\n",
    "    &emsp; Criar tópico Kafka para envio dos dados:\n",
    "</h2>\n",
    "<font size=3>\n",
    "    <ol>\n",
    "        <li>docker exec -it kafka bash</li>\n",
    "        <li>kafka-topics.sh --bootstrap-server localhost:9092 --topic exerc_3_basico_vis_3 --create --partitions 1 --replication-factor 1</li>\n",
    "        <li>kafka-topics.sh --bootstrap-server localhost:9092 --list</li>\n",
    "        <li>exit</li>\n",
    "    </ol>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    Sequência de comandos para execução no Jupyter-Notebook:<br>\n",
    "    &emsp; Exercício 6 Nível Básico:\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    1 - Salvar os dados em um tópico do Kafka:<br>\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data.write.format('kafka').option('kafka.bootstrap.servers', 'kafka:9092').option('topic', 'exerc_3_basico_vis_3').save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    Comandos de Terminal:<br>\n",
    "    &emsp; Criar consumidor Kafka para ler o tópico com os dados enviados:\n",
    "</h2>\n",
    "<font size=3>\n",
    "    <ol>\n",
    "        <li>docker exec -it kafka bash</li>\n",
    "        <li>kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic exerc_3_basico_vis_3 --from-beginning</li><br>\n",
    "        <li>Pressionar $<$Ctrl$>$+C</li><br>\n",
    "        <li>exit</li>\n",
    "    </ol>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    Sequência de comandos para execução no Jupyter-Notebook:<br>\n",
    "    &emsp; Exercício 8 Nível Básico:\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    1 - Ler os dados para a visualização aplicando o filtro necessário: \n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data = data_table.where(condition=(psf.col('casosAcumulado') > -1) & (psf.col('obitosAcumulado') > -1) & (psf.col('populacaoTCU2019') > -1) & (psf.col('data') != 'NullValue'))\\\n",
    "                        .select(psf.col('regiao'), psf.col('casosAcumulado'), psf.col('obitosAcumulado'), psf.col('populacaoTCU2019'), psf.col('data'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(regiao='Nordeste', casos=1717792, obitos=30844, populacao=14873064, data='29/11/2022')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_data = temp_data.groupBy('regiao')\\\n",
    "                        .agg(psf.max(psf.col('casosAcumulado')).alias('casos'), psf.max(psf.col('obitosAcumulado')).alias('obitos'), psf.max(psf.col('populacaoTCU2019')).alias('populacao'), psf.last(psf.col('data')).alias('data'))\n",
    "\n",
    "temp_data.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    2 - Incluir novas colunas entre os dados de acordo com a visualização:\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(regiao='Nordeste', casos=1717792, obitos=30844, populacao=14873064, data='29/11/2022', incidencia=11549.6845703125, mortalidade=207.38160705566406),\n",
       " Row(regiao='Sul', casos=2775035, obitos=45501, populacao=11433957, data='29/11/2022', incidencia=24270.119140625, mortalidade=397.94622802734375),\n",
       " Row(regiao='Sudeste', casos=6190288, obitos=176199, populacao=45919049, data='29/11/2022', incidencia=13480.8720703125, mortalidade=383.7165832519531),\n",
       " Row(regiao='Centro-Oeste', casos=1743272, obitos=27628, populacao=7018354, data='30/06/2022', incidencia=24838.7578125, mortalidade=393.653564453125),\n",
       " Row(regiao='Brasil', casos=35227599, obitos=689665, populacao=210147125, data='29/11/2022', incidencia=16763.3046875, mortalidade=328.1819763183594),\n",
       " Row(regiao='Norte', casos=851524, obitos=18918, populacao=8602865, data='29/11/2022', incidencia=9898.14453125, mortalidade=219.9034881591797)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_data = temp_data.withColumn(colName='incidencia', col=(psf.col('casos').cast('float')/psf.col('populacao').cast('float'))*100000.0)\n",
    "temp_data = temp_data.withColumn(colName='incidencia', col=psf.col('incidencia').cast('float'))\n",
    "\n",
    "temp_data = temp_data.withColumn(colName='mortalidade', col=(psf.col('obitos').cast('float')/psf.col('populacao').cast('float'))*100000.0)\n",
    "temp_data = temp_data.withColumn(colName='mortalidade', col=psf.col('mortalidade').cast('float'))\n",
    "\n",
    "temp_data.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>\n",
    "    3 - Salvar os dados da visualização no HDFS (Após erros em tentativas de enviar para Elasticsearch):\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data.write.csv(path='hdfs://namenode:8020/user/student/result_data/exercicio_8_data', sep=';', header=True, mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    Comandos de Terminal:<br>\n",
    "    &emsp; Verificar dados salvos no HDFS:\n",
    "</h2>\n",
    "<font size=3>\n",
    "    <ol>\n",
    "        <li>docker exec -it namenode bash</li>\n",
    "        <li>hdfs dfs -ls /user/student/result_data</li>\n",
    "        <li>hdfs dfs -ls /user/student/result_data/exercicio_8_data</li>\n",
    "        <li>exit</li>\n",
    "    </ol>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    Tentativa que funcionou para enviar os dados da visualização para o Elasticsearch:\n",
    "</h2>\n",
    "<h3>\n",
    "    Sequência de comandos para execução no Jupyter-Notebook, elaborados após tentativas sem sucesso de enviar direto para o Elasticsearch.\n",
    "</h3>\n",
    "<h3>\n",
    "    Os dados deverão ser incluídos em Índice do Elasticsearch através de importação do arquivo exercicio_8_data.csv (diretório $<$diretório-recursos-cluster-docker-projeto$>$/data/notebooks/) na interface Kibana.\n",
    "</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pandas_df = []\n",
    "for i, doc in enumerate(temp_data.collect()):\n",
    "    temp_doc = doc.asDict()\n",
    "    pandas_df.append(temp_doc)\n",
    "pandas_df = pd.DataFrame(pandas_df)\n",
    "\n",
    "pandas_df.to_csv('exercicio_8_data.csv', index=False, sep=';')\n",
    "del pandas_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    Tentativa 1 sem sucesso para enviar direto para o Elasticsearch:\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    Comandos de Terminal:\n",
    "</h2>\n",
    "<font size=3>\n",
    "    <ol>\n",
    "        <li>docker cp elasticsearch-spark-20_2.11-5.3.1.jar jupyter-spark:/opt/spark/jars</li><br>\n",
    "        <li>docker exec -it jupyter-spark bash</li>\n",
    "        <li>ls opt/spark/jars/</li>\n",
    "        <li>exit</li><br>\n",
    "        <li>docker compose stop</li>\n",
    "        <li>docker compose start</li>\n",
    "        <li>docker compose ps</li>\n",
    "    </ol>\n",
    "</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    Sequência de comandos para execução no Jupyter-Notebook:\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data.write.format('es').option('es.nodes', 'https://docker-projeto-elasticsearch-1:9200/').option('es.resource', 'exercicio_8_basico_index').save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    Tentativa 2 sem sucesso para enviar direto para o Elasticsearch:\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>\n",
    "    Sequência de comandos para execução no Jupyter-Notebook:\n",
    "</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.elastic.co/guide/en/elasticsearch/client/python-api/master/examples.html\n",
    "\n",
    "#Atualização de módulo após ocorrência de erros de requisitos do módulo elasticsearch.\n",
    "!pip install --upgrade requests\n",
    "!pip uninstall urllib3 --yes\n",
    "!pip install urllib3\n",
    "\n",
    "#Instalação de módulo para integração com Elasticsearch.\n",
    "!pip install elasticsearch\n",
    "\n",
    "\n",
    "\n",
    "import elasticsearch as els\n",
    "\n",
    "es = els.Elasticsearch('https://docker-projeto-elasticsearch-1:9200/')\n",
    "\n",
    "for i, doc in enumerate(temp_data.collect()):\n",
    "    resul = es.index(index='exercicio_8_basico_index', id=i+1, document=doc.asDict())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
